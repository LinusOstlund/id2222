{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Data Streams\n",
    "\n",
    "In this lab, we are supposed to pick a dataset and a paper. \n",
    "\n",
    "## Dataset\n",
    "We use the [Google Webgraph dataset](https://snap.stanford.edu/data/web-Google.html). Potential caveat that it contains *directed* edges. \n",
    "\n",
    "| **Property** | **Value** |\n",
    "| --- | --- |\n",
    "| Nodes | 875713          |\n",
    "| Edges | 5105039         |\n",
    "| Nodes in largest WCC | 855802 (0.977)  |\n",
    "| Edges in largest WCC | 5066842 (0.993) |\n",
    "| Nodes in largest SCC | 434818 (0.497)  |\n",
    "| Edges in largest SCC | 3419124 (0.670) |\n",
    "| Average clustering coefficient   | 0.5143          |\n",
    "| Number of triangles              | 13391903        |\n",
    "| Fraction of closed triangles     | 0.01911         |\n",
    "| Diameter (longest shortest path) | 21              |\n",
    "| 90-percentile effective diameter | 8.1             |\n",
    "\n",
    "## Paper\n",
    "* ~~We chose the algorithm in the paper [A Space-Efficient Streaming Algorithm for Estimating Transitivity and Triangle Counts Using the Birthday Paradox](https://arxiv.org/pdf/1212.2264.pdf) by M. Jha, C. Seshadhri, and A. Pinar.~~\n",
    "* Second thought, how about no. Terrible psuedocode made us turn to [TRIÃˆST: Counting Local and Global Triangles in\n",
    "Fully-Dynamic Streams with Fixed Memory Size](https://www.kdd.org/kdd2016/papers/files/rfp0465-de-stefaniA.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run this Notebook with Docker\n",
    "If you want to run this Notebook with Docker, the Jupyter team has several images over at ... . In the [documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html) they show an example:\n",
    "\n",
    "```bash\n",
    "docker run -p 10000:8888 jupyter/pyspark-notebook:latest\n",
    "```\n",
    "\n",
    "> Visiting `http://<hostname>:10000/?token=<token>` in a browser loads JupyterLab, where `<hostname> = 0.0.0.0` and token is generated in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4bbcff5a96aa:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff88fda500>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"lab3\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a csv file with pyspark\n",
    "rdd = spark.read.option(\"delimiter\", \"\\t\").csv(\"../../data/web-Google.txt\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|FromNodeId|ToNodeId|\n",
      "+----------+--------+\n",
      "|         0|   11342|\n",
      "|         0|  824020|\n",
      "|         0|  867923|\n",
      "|         0|  891835|\n",
      "|     11342|       0|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show df schema\n",
    "rdd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know how to simulate a stream properly, so I'll convert the rdd to a Panda's dataframe, and iterate over each row ðŸ¤·ðŸ»â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df = rdd.toPandas()\n",
    "stream = []\n",
    "for index, row in df.iterrows():\n",
    "    edge = (row[0], row[1])\n",
    "    stream.append(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in the paper:\n",
    "\n",
    "* Our algorithms keep an edge sample $\\mathcal{S}$ of up to $M$ edges from the stream (as contrary to sampling with a probability $p$)\n",
    "\n",
    "* TriÃ¨st algorithms keep counters to compute the estimations of the global and local number of triangles. They always keep one global counter $\\tau$ for the estimation of the global number of triangles.\n",
    "\n",
    "* For any t â‰¥ 0, let $G^S = (V^S, E^S)$ be the subgraph of $G(t)$ containing all and only the edges in the currentsample $\\mathcal{S}$. We denote with $\\mathcal{N}^S_u$ the neighborhood of $u$ in $G^S: \\mathcal{N}^S_u = \\{v âˆˆ V^{(t)}: (u, v) âˆˆ \\mathcal{S}\\}$ and with $\\mathcal{N}^\\mathcal{S}_{u, v} = \\mathcal{N}^\\mathcal{S}_u \\cap \\mathcal{N}^\\mathcal{S}_v$ the shared neighborhood of $u$ and $v$ in $G^\\mathcal{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "from operator import add, sub\n",
    "\n",
    "class TriestBase:\n",
    "    # set up nasty globals >_<\n",
    "    def __init__(self, stream, M=6):\n",
    "        self.stream = stream\n",
    "        # defaultdict avoids missing key errors, and with 'int' it defaults to 0\n",
    "        self.tau = defaultdict(int)\n",
    "        self.S = set()\n",
    "        self.t = 0\n",
    "        self.M = M\n",
    "\n",
    "    def run(self):\n",
    "        for edge in self.stream:\n",
    "            self.t+=1\n",
    "            if self.sample_edge(edge):\n",
    "                self.S = self.S ^ {edge}\n",
    "                self.update_counters(operator=add, edge=edge) \n",
    "\n",
    "    def sample_edge(self, edge):\n",
    "        if self.t <= self.M:\n",
    "            return True\n",
    "        elif self.flip_biased_coin(self.M/self.t):\n",
    "            # pick a random edge from S\n",
    "            random_edge = sample(self.S, 1) # set of tuples require sample?\n",
    "            # remove it from S\n",
    "            self.S = self.S - {random_edge[0]}\n",
    "            self.update_counters(operator=sub, edge=random_edge[0])\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def flip_biased_coin(self, p):\n",
    "        # TODO is olikheten correct or should it be '>'?\n",
    "        return np.random.rand() < p\n",
    "\n",
    "    def update_counters(self, operator, edge):\n",
    "        u, v = edge\n",
    "        # TODO is this really right?\n",
    "        neighborhood = set(self.neighbors(u)) & set(self.neighbors(v))\n",
    "        for c in neighborhood:\n",
    "            # bug here, the operator is malfunctioning???\n",
    "            self.tau['global'] = operator(self.tau['global'], 1)\n",
    "            self.tau[c] = operator(self.tau[c], 1)\n",
    "            self.tau[u] = operator(self.tau[u], 1)\n",
    "            self.tau[v] = operator(self.tau[v], 1)\n",
    "\n",
    "    def neighbors(self, vertex):\n",
    "        # if this is a bottle neck, it could be optimized by array indexing\n",
    "        for u, v in self.S:\n",
    "            if u != vertex:\n",
    "                continue\n",
    "            else:\n",
    "                yield v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136/1551994115.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  sample(test_set, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "test_set = {(1,2), (3,4), (5,6)}\n",
    "sample(test_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136/1961716842.py:28: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_edge = sample(self.S, 1) # set of tuples require sample?\n"
     ]
    }
   ],
   "source": [
    "tb = TriestBase(stream, M=100)\n",
    "tb.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-152"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for v in tb.tau.values():\n",
    "    sum += v\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import add, sub   \n",
    "\n",
    "d = defaultdict(int)\n",
    "f = add\n",
    "f(d['a'], 1)\n",
    "\n",
    "f = sub\n",
    "print(f(d['b'], 1))\n",
    "f(d['b'], 1)\n",
    "f(d['b'], 1)\n",
    "\n",
    "d['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1), (0, 2), (1, 2), (1, 3), (3, 4)}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = {(0,1), (0,2), (1,2), (3,4)}\n",
    "b = (1,3)\n",
    "\n",
    "test_graph ^ {b}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
